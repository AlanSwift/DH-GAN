{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Meta information\n",
    "* train instances: \"/home/shiina/data/nips/vqa2/annotation/cache/train_instances_with_doublehints.pkl\"\n",
    "* val instances: \"/home/shiina/data/nips/vqa2/annotation/cache/val_instances_with_doublehints.pkl\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([300])\n"
     ]
    }
   ],
   "source": [
    "import torch, copy\n",
    "import torchtext.vocab as vocab\n",
    "glove_text = vocab.GloVe(name='6B', dim=300)\n",
    "print(glove_text[\"animal\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<stanfordcorenlp.corenlp.StanfordCoreNLP object at 0x7f27c83954c0>\n"
     ]
    }
   ],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "nlp_parser = StanfordCoreNLP('http://localhost', port=9000, timeout=300000)\n",
    "print(nlp_parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<stanfordcorenlp.corenlp.StanfordCoreNLP object at 0x7f27c83954c0>\nTrue\nTrue\n"
     ]
    }
   ],
   "source": [
    "print(nlp_parser)\n",
    "vg_replace_dict = {\n",
    "    \"ceiling fan\": \"fan\",\n",
    "    \"birthday cake\": \"cake\",\n",
    "    \"skateboard ramp\": \"ramp\",\n",
    "    \"towel rack\": \"rack\",\n",
    "    \"tree branch\": \"branch\",\n",
    "    \"tile floor\": \"floor\",\n",
    "    \"ski jacket\": \"anorak\",\n",
    "    \"tennis court\": \"court\",\n",
    "    \"rock wall\": \"wall\",\n",
    "    \"tennis racket,tennis racquet\": \"racquet\",\n",
    "    \"toilet brush\": \"brush\",\n",
    "    \"wii remote\": \"remote-control\",\n",
    "    \"brocolli\": \"broccoli\",\n",
    "    \"sandwhich\": \"sandwich\",\n",
    "    \"skiis\": \"skis\",\n",
    "    \"kneepad\": \"kneecap\"\n",
    "}\n",
    "print(\"kneecap\" in glove_text.stoi)\n",
    "print(\"skis\" in glove_text.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2222\n2222\n2222\n2222\n2222\n2222\n2222\n2222\n2222\n2222\n2222\n2222\n2222\n2222\n"
     ]
    }
   ],
   "source": [
    "\n",
    "similarity_func = glove_text\n",
    "\n",
    "def load_class():\n",
    "    ret = {}\n",
    "    with open(\"/home/shiina/shiina/question/aaai/preprocess/vqa2/vg_class.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, cls in enumerate(lines):\n",
    "            cls = cls.strip()\n",
    "#             print(idx, cls)\n",
    "            candidate = cls.split(\",\")\n",
    "            is_find = False\n",
    "            for can in candidate:\n",
    "                \n",
    "                can_ = can.replace(\" \", \"-\")\n",
    "                can_noblank = can.replace(\" \", \"\")\n",
    "                if can in similarity_func.stoi:\n",
    "                    ret[idx] = [can]\n",
    "                    is_find = True\n",
    "                    # print(\"11111\")\n",
    "                    break\n",
    "                if can_ in similarity_func.stoi:\n",
    "                    ret[idx] = [can_]\n",
    "                    is_find = True\n",
    "                    # print(\"11111\")\n",
    "                    break\n",
    "                if can_noblank in similarity_func.stoi:\n",
    "                    ret[idx] = [can_noblank]\n",
    "                    is_find = True\n",
    "                    # print(\"11111\")\n",
    "                    break\n",
    "                elif can in vg_replace_dict.keys():\n",
    "                    ret[idx] = vg_replace_dict[can]\n",
    "                    is_find = True\n",
    "                    print(\"2222\")\n",
    "                    break\n",
    "            if not is_find:\n",
    "                collect = []\n",
    "                for can in candidate:\n",
    "                    collect.extend(can.split(\" \"))\n",
    "                valid_collect = []\n",
    "                for can in collect:\n",
    "                    if can in similarity_func.stoi:\n",
    "                        valid_collect.append(can)\n",
    "                if len(valid_collect) < 1:\n",
    "                    print(idx, cls, \"*******\")\n",
    "                ret[idx] = valid_collect\n",
    "    return ret\n",
    "\n",
    "#     return ret\n",
    "vg_idx2cls = load_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_similarity(a, b):\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.from_numpy(a).view(-1)\n",
    "        b = torch.from_numpy(b).view(-1)\n",
    "    diff = a - b\n",
    "    l2 = torch.norm(diff, dim=0)\n",
    "    return l2\n",
    "\n",
    "def l2_similarity_2d(a, b):\n",
    "    assert len(a.shape) == 2\n",
    "    assert len(b.shape) == 2\n",
    "    assert a.shape[1] == b.shape[1]\n",
    "    a = a.unsqueeze(1) # N1, 1, D\n",
    "    b = b.unsqueeze(0) # 1, N2, D\n",
    "    diff = a - b\n",
    "    l2 = torch.norm(diff, dim=2)\n",
    "    return l2\n",
    "\n",
    "similarity_func = glove_text\n",
    "\n",
    "def similarity(candidate_word, reference_word_list, threshold=5.7):\n",
    "    ret = []\n",
    "    for ref in reference_word_list:\n",
    "        ref_vectors = [similarity_func[ref_word].unsqueeze(0) for ref_word in ref]\n",
    "        ref_vectors = torch.cat(ref_vectors, dim=0)\n",
    "        candidate_vector = vector = similarity_func[candidate_word].unsqueeze(0)\n",
    "        similarity_matrix = l2_similarity_2d(candidate_vector, ref_vectors)\n",
    "        most_similar_value = similarity_matrix[0, :].min()\n",
    "        if most_similar_value.item() <= threshold:\n",
    "            ret.append(1)\n",
    "        else:\n",
    "            ret.append(0)\n",
    "    if len(ret) < 36:\n",
    "        ret.extend([0 for _ in range(36 - len(ret))])\n",
    "    return torch.Tensor(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, torch, collections, json\n",
    "\n",
    "def stanfordcorenlp_pos_tag(sentence: str, nlp_processor):\n",
    "    sentence = re.sub('\\.+', r'.', sentence)\n",
    "    sentence = re.sub('([a-z])([.,!?()])', r'\\1 \\2 ', sentence)\n",
    "    sentence = re.sub('\\?', ' ', sentence)\n",
    "    sentence = re.sub('\\s+', ' ', sentence)\n",
    "    props = {\n",
    "        'annotators': 'ssplit,tokenize,pos,lemma',\n",
    "        \"tokenize.options\":\n",
    "            \"splitHyphenated=false,normalizeParentheses=false,normalizeOtherBrackets=false\",\n",
    "        \"tokenize.whitespace\": False,\n",
    "        'ssplit.isOneSentence': True,\n",
    "        'outputFormat': 'json'\n",
    "    }\n",
    "    ret = nlp_processor.annotate(sentence.lower(), props)\n",
    "    try:\n",
    "        pos_dict = json.loads(ret)['sentences'][0]['tokens']\n",
    "\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "    ret = []\n",
    "    for token in pos_dict:\n",
    "        ret.append({\n",
    "            \"word_idx\": token['index'] - 1,\n",
    "            \"word\": token[\"word\"],\n",
    "            \"lemma\": token[\"lemma\"],\n",
    "            \"pos\": token[\"pos\"]\n",
    "        })\n",
    "    return ret\n",
    "\n",
    "\n",
    "def align_sentence_bbox(parsed_sentence, bbox, threshold=5.7):\n",
    "\n",
    "    bbox_cls = bbox[\"object_id\"].reshape(-1).tolist()\n",
    "#     print(bbox_cls)\n",
    "\n",
    "    # convert class to vectors\n",
    "#     print(vg_idx2cls[0])\n",
    "\n",
    "    bbox_cls_name = [vg_idx2cls[bbox_cls_id] for bbox_cls_id in bbox_cls]\n",
    "    \n",
    "#     print(bbox_cls_name)\n",
    "\n",
    "#     bbox_vectors = [class_vectors[bbox_cls_id].unsqueeze(0) for bbox_cls_id in bbox_cls]\n",
    "#     bbox_vectors = torch.cat(bbox_vectors, dim=0)\n",
    "\n",
    "    parsed_sentence = copy.deepcopy(parsed_sentence)\n",
    "\n",
    "    mask = torch.zeros(36)\n",
    "    for word_inst in parsed_sentence:\n",
    "        word = word_inst[\"word\"]\n",
    "        lemma = word_inst[\"lemma\"]\n",
    "        pos = word_inst[\"pos\"]\n",
    "        if pos not in [\"PRP\", \"PRP$\"] and pos[:1] != \"N\":\n",
    "            continue\n",
    "\n",
    "        if pos in [\"PRP\", \"PRP$\"]:\n",
    "            if lemma in [\"he\", \"she\"]:\n",
    "                word = \"person\"\n",
    "                lemma = \"person\"\n",
    "        if lemma in [\"he\", \"she\", \"boy\", \"girl\", \"his\", \"her\", \"woman\", \"man\", \"people\", \"crowd\", \"lady\", \"gentleman\"]:\n",
    "            lemma = \"person\"\n",
    "\n",
    "        if lemma in similarity_func.stoi:\n",
    "            valid_mask = similarity(lemma, bbox_cls_name, threshold)\n",
    "            mask += valid_mask\n",
    "    return mask > 0\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "def visualize(image_path, ppl_path):\n",
    "    src_img = cv2.imread(image_path)\n",
    "    \n",
    "    with open(ppl_path, \"rb\") as f:\n",
    "        ppl = pickle.load(f)\n",
    "    \n",
    "    for i in range(36):\n",
    "        if ppl[\"confidence\"][i][0] <= 0.4:\n",
    "            break\n",
    "        coor = ppl[\"original_spatial_features\"][i, :]\n",
    "        x1, y1, x2, y2 = coor\n",
    "        cv2.rectangle(src_img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), thickness=2)\n",
    "    \n",
    "    bbox_cls = ppl[\"obj_cls\"].reshape(-1).tolist()\n",
    "\n",
    "    bbox_cls_name = [vg_idx2cls[bbox_cls_id] for bbox_cls_id in bbox_cls]\n",
    "    print(bbox_cls_name)\n",
    "    \n",
    "    print(ppl[\"confidence\"])\n",
    "\n",
    "    output_name = \"/home/shiina/output/test.jpg\"\n",
    "    cv2.imwrite(output_name, src_img)\n",
    "    pass\n",
    "\n",
    "MAX_VH = 4\n",
    "\n",
    "def annotate(split: collections.defaultdict, threshold=5.7):\n",
    "    ret = {}\n",
    "    cnt = 0\n",
    "    for qid, inst in tqdm(split.items()):\n",
    "        question = inst[\"question\"].lower()\n",
    "        answer = inst[\"multiple_choice_answer\"].lower()\n",
    "        img_id = inst[\"image_id\"]\n",
    "\n",
    "        parsed_results = stanfordcorenlp_pos_tag(question, nlp_parser)\n",
    "        if not parsed_results:\n",
    "            continue\n",
    "        cnt += 1\n",
    "        ppl_path = inst[\"ppl_path\"]\n",
    "        with open(ppl_path, \"rb\") as f:\n",
    "            bbox = pickle.load(f)\n",
    "        \n",
    "        question_valid_mask = align_sentence_bbox(parsed_results, bbox, threshold=threshold)\n",
    "        question_tokens = [token_inst[\"word\"] for token_inst in parsed_results]\n",
    "        question_lemmas = [token_inst[\"lemma\"] for token_inst in parsed_results]\n",
    "        \n",
    "        \n",
    "        parsed_results = stanfordcorenlp_pos_tag(inst[\"multiple_choice_answer\"].lower(), nlp_parser)\n",
    "        if not parsed_results:\n",
    "            continue\n",
    "        answer_valid_mask = align_sentence_bbox(parsed_results, bbox, threshold=threshold)\n",
    "        answer_tokens = [token_inst[\"word\"] for token_inst in parsed_results]\n",
    "        answer_lemmas = [token_inst[\"lemma\"] for token_inst in parsed_results]\n",
    "\n",
    "        valid_mask = (question_valid_mask.float() + answer_valid_mask.float()) > 0\n",
    "        valid_mask = valid_mask.float()\n",
    "        \n",
    "        if valid_mask.sum() > MAX_VH:\n",
    "#             print(img_id)\n",
    "#             print(question)\n",
    "#             print(answer)\n",
    "#             print(valid_mask)\n",
    "#             image_id_str = str(img_id).zfill(12)\n",
    "#             visualize(os.path.join(\"/home/shiina/data/coco2014/train2014/pic\", \"COCO_train2014_{}.jpg\".format(image_id_str)), ppl_path)\n",
    "#             break\n",
    "            # we will select at most 4 proposals, considering both diversity and confidence\n",
    "#             print(\"before prune: \", valid_mask)\n",
    "            active_list = torch.where(valid_mask == 1)\n",
    "            active_list = active_list[0]\n",
    "#             print(active_list, \"ppppppp\")\n",
    "            \n",
    "            confidence = bbox[\"confidence\"]\n",
    "            \n",
    "            bbox_cls = bbox[\"object_id\"].reshape(-1).tolist()\n",
    "            \n",
    "            candidate_score = [confidence[idx, 0] for idx in active_list]\n",
    "            candidate_cls = copy.deepcopy([bbox_cls[idx] for idx in active_list])\n",
    "#             print(\"candidate_score: \", candidate_score)\n",
    "#             print(\"candidate_cls: \", candidate_cls)\n",
    "            \n",
    "            \n",
    "            tobeselected_cls = list(set(candidate_cls))[:MAX_VH]\n",
    "#             print(\"tobeselected_cls: \", tobeselected_cls)\n",
    "            \n",
    "            chosen = []\n",
    "            \n",
    "            for cls in tobeselected_cls:\n",
    "                for i in range(len(active_list)):\n",
    "                    if candidate_cls[i] == cls:\n",
    "                        chosen.append(active_list[i])\n",
    "                        candidate_cls[i] = -1\n",
    "                        break\n",
    "            if len(chosen) < MAX_VH:\n",
    "                for i in range(len(active_list)):\n",
    "                    if candidate_cls[i] == -1:\n",
    "                        continue\n",
    "                    chosen.append(active_list[i])\n",
    "                    if len(chosen) >= MAX_VH:\n",
    "                        break\n",
    "            valid_mask = torch.zeros(36)\n",
    "            for i in chosen:\n",
    "                valid_mask[i] = 1\n",
    "        \n",
    "        if valid_mask.sum() == 0 and answer_tokens != [\"no\"]:\n",
    "            continue\n",
    "        else:\n",
    "            \n",
    "            inst[\"question_tokens\"] = question_tokens\n",
    "            inst[\"question_lemmas\"] = question_lemmas\n",
    "            inst[\"answer_tokens\"] = answer_tokens\n",
    "            inst[\"answer_lemmas\"] = answer_lemmas\n",
    "            inst[\"visual_hint\"] = valid_mask.numpy().tolist()\n",
    "            ret[qid] = copy.deepcopy(inst)\n",
    "    \n",
    "    print(\"Statistics: before processing: {}, after processing: {}\".format(cnt, len(ret.keys())))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 443757/443757 [1:25:54<00:00, 86.10it/s]\n",
      "Statistics: before processing: 443757, after processing: 355157\n",
      "finish, save to /home/shiina/data/nips/vqa2/annotation/cache/train_instances_with_doublehints.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle, os\n",
    "\n",
    "train_input_instances_path = \"/home/shiina/data/nips/vqa2/annotation/raw/train_instances.pkl\"\n",
    "train_annotation_save_path = \"/home/shiina/data/nips/vqa2/annotation/cache/train_instances_with_doublehints.pkl\"\n",
    "\n",
    "os.makedirs(os.path.split(train_annotation_save_path)[0], exist_ok=True)\n",
    "\n",
    "with open(train_input_instances_path, \"rb\") as f:\n",
    "    train_instances = pickle.load(f)\n",
    "annotated_data = annotate(split=train_instances)\n",
    "\n",
    "with open(train_annotation_save_path, \"wb\") as f:\n",
    "    pickle.dump(annotated_data, f)\n",
    "print(\"finish, save to {}\".format(train_annotation_save_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 214354/214354 [40:48<00:00, 87.56it/s]\n",
      "Statistics: before processing: 214354, after processing: 171978\n",
      "finish, save to /home/shiina/data/nips/vqa2/annotation/cache/val_instances_with_doublehints.pkl\n"
     ]
    }
   ],
   "source": [
    "val_input_instances_path = \"/home/shiina/data/nips/vqa2/annotation/raw/val_instances.pkl\"\n",
    "val_annotation_save_path = \"/home/shiina/data/nips/vqa2/annotation/cache/val_instances_with_doublehints.pkl\"\n",
    "\n",
    "os.makedirs(os.path.split(val_annotation_save_path)[0], exist_ok=True)\n",
    "\n",
    "with open(val_input_instances_path, \"rb\") as f:\n",
    "    val_instances = pickle.load(f)\n",
    "val_annotated_data = annotate(split=val_instances)\n",
    "\n",
    "with open(val_annotation_save_path, \"wb\") as f:\n",
    "    pickle.dump(val_annotated_data, f)\n",
    "print(\"finish, save to {}\".format(val_annotation_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}